{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Trong các bài toán Máy học liên quan đến dữ liệu văn bản (text), tiền xử lý văn bản (text\n",
    "preprocessing) là một kỹ thuật cực kì quan trọng, góp phần làm cải thiện khả năng biểu diễn,\n",
    "phân tích ngôn ngữ và trên hết là gia tăng hiệu suất mô hình. Một trong số các kỹ thuật tiền xử\n",
    "lý văn bản phổ biến gồm có:\n",
    "\n",
    "(a) **Chuyển chữ viết thường (lowercasing)**: Vì máy tính coi hai chữ cái `a` và `A` là hai\n",
    "ký tự khác nhau mặc dù có cùng ý nghĩa, điều này làm gia tăng độ phức tạp cũng như tài\n",
    "nguyên lưu trữ khi biểu diễn văn bản. Lowercasing là kỹ thuật chuyển toàn bộ các chữ cái\n",
    "trong đoạn văn bản thành kiểu chữ viết thường, sử dụng lowercasing sẽ phần nào giúp ta\n",
    "tránh được trường hợp kể trên.\n",
    "\n",
    "(b) **Chuyển chữ viết hoa (uppercasing)**: Với mục đích tương tự như kỹ thuật lowercasing,\n",
    "ở kỹ thuật uppercasing các chữ cái có trong văn bản sẽ được chuyển toàn bộ sang kiểu chữ\n",
    "viết hoa.\n",
    "\n",
    "(c) **Loại bỏ đường dẫn URL (URL removal)**: là kỹ thuật loại bỏ các đường dẫn liên kết\n",
    "đến website có trong văn bản. Mục đích chính của việc loại bỏ các url đó là vì chúng không thật sự biểu diễn đúng ngôn ngữ tự nhiên, vì vậy nhiều khả năng sẽ không chứa những thông\n",
    "tin quan trọng trong việc biểu diễn văn bản.\n",
    "\n",
    "(d) **Loại bỏ các thẻ html (html tags removal)**: là kỹ thuật loại bỏ các chuỗi có dấu `<>` ở\n",
    "đầu cuối. Đây là các chuỗi có dạng là các thẻ html. Kỹ thuật này sẽ cực kỳ hữu ích khi bộ\n",
    "dữ liệu của chúng ta là các file .html, hoặc văn bản trích dẫn... Gợi ý: các bạn có thể tìm hiểu thư viện [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) (sử dụng phương thức ``get_text()``\n",
    "của class ``BeautifulSoup()``) để thực hiện kỹ thuật này.\n",
    "\n",
    "(e) **Loại bỏ dấu câu (punctuations removal)**: là kỹ thuật loại bỏ đi các dấu câu như\n",
    "``! ? : ; < > ...`` có trong văn bản (nằm ngoài hoặc nằm bên trong một từ). Việc loại\n",
    "bỏ dấu câu có mục đích giảm bớt sự biểu diễn các từ không cần thiết tương tự với kỹ thuật\n",
    "lowercasing, uppercasing.\n",
    "\n",
    "(f) **Loại bỏ stopwords (stopwords removal)**: Là kỹ thuật loại bỏ các từ \"stopwords\" có\n",
    "trong văn bản. Stopwords là một tập các từ được các nhà khoa học nghiên cứu và định nghĩa\n",
    "là các từ không mang nhiều thông tin quan trọng trong quá trình phân tích văn bản, vì vậy\n",
    "có thể loại bỏ chúng khi biểu diễn văn bản. Gợi ý: các bạn có thể truy cập vào bộ stopwords sử dụng thư viện nltk ([link](https://www.nltk.org/data.html)).\n",
    "\n",
    "(g) **Loại bỏ các từ có tần suất xuất hiện nhiều nhất (frequent words removal)**: trong\n",
    "một số trường hợp, việc loại bỏ các từ xuất hiện quá nhiều trong bộ dữ liệu (bộ dữ liệu sẽ\n",
    "bao gồm rất nhiều văn bản khác nhau) sẽ cải thiện hiệu suất mô hình. Kỹ thuật frequent\n",
    "words removal sẽ loại bỏ k (k > 0) từ có tần suất xuất hiện cao nhất. Trong phạm vi bài\n",
    "tập này, ta sẽ thực hiện kỹ thuật này dưới mức độ trên một đoạn văn bản duy nhất (tương\n",
    "đương với bộ dữ liệu chỉ có một văn bản).\n",
    "\n",
    "(h) **Sửa lỗi chính tả (spelling correction)**: là kỹ thuật thay thế các từ sai chính tả trong\n",
    "văn bản thành từ đúng. Gợi ý: các bạn có thể sử dụng class ``Speller()`` trong thư viện\n",
    "autocorrect ([link](https://github.com/filyp/autocorrect)) để thực hiện kỹ thuật này.\n",
    "\n",
    "(i) **Stemming**: trong tiếng anh, một số từ thường sẽ thay đổi đuôi của từ gốc của chúng dựa\n",
    "theo từ loại (intelligence, intelligent...) hoặc thì đối với động từ (change, changes, changing,\n",
    "changed...). Điều này dẫn đến một ý tưởng kỹ thuật đưa tất cả các từ trong văn bản về dạng\n",
    "gốc của chúng (stem form) nhằm biểu diễn văn bản hiệu quả và đỡ phức tạp hơn dựa trên\n",
    "một tập các luật đổi từ do các nhà khoa học tạo ra. Gợi ý: các bạn có thể sử dụng class ``PorterStemmer()`` trong thư viện nltk ([link](https://www.nltk.org/data.html)) để thực hiện kỹ thuật này.\n",
    "\n",
    "(j) **Lemmatization**: tương tự với mục đích ở kỹ thuật stemming, lemmatization cũng là một\n",
    "kỹ thuật đưa các từ trong văn bản về dạng gốc của nó (trong tiếng anh) nhưng theo cách\n",
    "phức tạp hơn. Gợi ý: các bạn hãy sử dụng thuộc tính lemma_ của các token mà spacy model\n",
    "tính được từ văn bản đầu vào trong thư viện spacy ([link](https://spacy.io/usage/linguistic-features)). \n",
    "\n",
    "Với khái niệm của các kỹ thuật trên, các bạn hãy viết chương trình Python khai báo class ``TextPreprocessor()`` với một số yêu cầu sau:\n",
    "* Trong phương thức khởi tạo (constructor), cài đặt các thuộc tính về ``stopwords``, ``stemmer`` ``spelling_correction``.\n",
    "* Khai báo các phương thức mô phỏng các kỹ thuật tiền xử lý đã miêu tả ở đầu bài, với input\n",
    "là văn bản cần xử lý và output là văn bản đã được xử lý.\n",
    "* Khai báo một phương thức có tên ``preprocess_text()``, đây là phương thức sẽ áp dụng các\n",
    "kỹ thuật tiền xử lý đã khai báo trước đó:\n",
    "    * Bộ tham số đầu vào của phương thức gồm có:\n",
    "        * 1 tham số bắt buộc text. Tham số này chứa văn bản cần tiền xử lý.\n",
    "        * 10 tham số mặc định bao gồm ``lowercase``, ``uppercase``, ``remove_punct``, ``remove_stopwords``, ``remove_freqs``, ``stemming``, ``lemmatize``, ``remove_url``, ``remove_tags``, ``word_correct``. Đây là các tham số giúp người dùng có thể chỉ định kỹ thuật họ muốn sử dụng (các bạn tùy chọn giá trị mặc định cho các tham số này).\n",
    "    * Chỉ cho phép người dùng sử dụng một trong hai kỹ thuật ``lowercasing`` hoặc\n",
    "``uppercasing``. Nếu người dùng chỉ định sử dụng cả hai, in ra dòng thông báo lỗi ``Cannot\n",
    "do both lowercasing and uppercasing. Please specify only one of them.`` và trả\n",
    "về giá trị ``None``.\n",
    "    *Chỉ cho phép người dùng sử dụng một trong hai kỹ thuật stemming hoặc\n",
    "lemmatization. Nếu người dùng chỉ định sử dụng cả hai, in ra dòng thông báo lỗi\n",
    "``Cannot do both stemming and lemmatization. Please specify only one of\n",
    "them.`` và trả về giá trị ``None``.\n",
    "    * Với các kỹ thuật được người dùng chỉ định sử dụng, áp dụng chúng lên văn bản đầu vào\n",
    "và cuối cùng trả về văn bản đã được xử lý. Lưu ý: áp dụng các kỹ thuật theo thứ tự\n",
    "liệt kê ở đầu bài.\n",
    "\n",
    "* Như vậy, Input/Output của chương trình là (sau khi đã khai báo đối tượng ``TextPreprocessor()`` và bắt đầu gọi phương thức ``preprocess_text()``):\n",
    "    *  Input:\n",
    "        * text (bắt buộc).\n",
    "        * ``lowercase``, ``uppercase``, ``remove_punct``, ``remove_stopwords``, ``remove_freqs``, ``stemming``, ``lemmatize``, ``remove_url``, ``remove_tags``, ``word_correct`` (không bắt buộc).\n",
    "    * Output: Văn bản đã được tiền xử lý.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in c:\\programdata\\anaconda3\\lib\\site-packages (4.8.0)\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "# Uncomment to install:\n",
    "#\n",
    "# !pip install spacy==2.3.5\n",
    "# !pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz\n",
    "# !pip install pyresparser\n",
    "# !pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "# EXERCISE 1:\n",
    "#\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "nltk.download(\"stopwords\")\n",
    "load_model = spacy.load('en_core_web_sm', disable = ['parser','ner'])\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer as ps\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self, stopwords = None, stemmer = None, lemmatizer = None, spelling_correction = None):\n",
    "        self.stopwords = stopwords\n",
    "        self.stemmer = stemmer\n",
    "        self.lemmatizer = lemmatizer\n",
    "        self.spelling_correction = spelling_correction\n",
    "    \n",
    "    def lowercase(self, text):\n",
    "        return text.lower()\n",
    "    \n",
    "    def upper(self, text):\n",
    "        return text.upper()\n",
    "    \n",
    "    def remove_punct(self, text):\n",
    "        return text.translate(text.maketrans('','', string.punctuation))\n",
    "\n",
    "    def remove_stopwords(self, text):\n",
    "        stopwords_list = list(stopwords.words(\"english\"))\n",
    "        return text.replace([word for word in stopwords_list], '')\n",
    "    \n",
    "    def remove_freqs(self, text):\n",
    "        words_counts = Counter(text)\n",
    "        most_frequent_word = max(words_counts, key = words_counts.get)\n",
    "        return text.replace(most_frequent_word, '')\n",
    "\n",
    "    def stemming(self, text):\n",
    "        words_list = text.split()\n",
    "        stem_words_list = []\n",
    "        for word in words_list:\n",
    "            stem_word = ps.stem(word)\n",
    "            stem_words_list.append(stem_word)\n",
    "        return ''.join(stem_words_list)\n",
    "    \n",
    "    def lemmatization(self, text):\n",
    "        doc = load_model(text)\n",
    "        return ''.join([token.lemma_ for token in doc])\n",
    "\n",
    "    def remove_url(self, text):\n",
    "        return re.sub(r'http\\S+','', text) # Use Regular Expression\n",
    "    \n",
    "    def remove_tags(self, text):\n",
    "        return BeautifulSoup(text, 'lxml').text\n",
    "\n",
    "    def preprocess_text(self, text, lowercase = False, uppercase = False, remove_punct = False, remove_stopwords = False, stemming = False, lemmatization = False, remove_url = False, remove_tags = False):\n",
    "        preprocessed_text = text\n",
    "\n",
    "        assert(lowercase == True and uppercase != True) or (lowercase != True and uppercase == True),'Cannot do both lowercasing and uppercasing. Please specify only one of them.'\n",
    "        if lowercase == True:\n",
    "            preprocessed_text = self.lowercase(preprocessed_text)\n",
    "\n",
    "        if uppercase == True:\n",
    "            preprocessed_text = self.lowercase(preprocessed_text)\n",
    "        \n",
    "        if remove_punct == True:\n",
    "            preprocessed_text = self.remove_punct(preprocessed_text)\n",
    "\n",
    "        if remove_stopwords == True:\n",
    "            preprocessed_text = self.remove_stopwords(preprocessed_text)\n",
    "\n",
    "        assert(stemming == True and lemmatization != True) or (stemming != True and lemmatization == True),'Cannot do both stemming and lemmatization. Please specify only one of them.'\n",
    "        if stemming == True:\n",
    "            preprocessed_text = self.stemming(preprocessed_text)\n",
    "\n",
    "        if lemmatization == True:\n",
    "            preprocessed_text = self.lemmatization(preprocessed_text)\n",
    "        \n",
    "        if remove_url == True:\n",
    "            preprocessed_text = self.remove_url(preprocessed_text)\n",
    "        \n",
    "        if remove_tags == True:\n",
    "            preprocessed_text = self.remove_tags(preprocessed_text)\n",
    "\n",
    "        return preprocessed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Cannot do both lowercasing and uppercasing. Please specify only one of them.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Administrator\\Documents\\GitHub\\aio-2022\\assignment\\solutions\\quang-nguyen_vo-huynh_assignment_6.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Documents/GitHub/aio-2022/assignment/solutions/quang-nguyen_vo-huynh_assignment_6.ipynb#ch0000006?line=3'>4</a>\u001b[0m preprocessor \u001b[39m=\u001b[39m TextPreprocessor()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Documents/GitHub/aio-2022/assignment/solutions/quang-nguyen_vo-huynh_assignment_6.ipynb#ch0000006?line=4'>5</a>\u001b[0m text \u001b[39m=\u001b[39m \\\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Documents/GitHub/aio-2022/assignment/solutions/quang-nguyen_vo-huynh_assignment_6.ipynb#ch0000006?line=5'>6</a>\u001b[0m \u001b[39m\"\"\" \u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Documents/GitHub/aio-2022/assignment/solutions/quang-nguyen_vo-huynh_assignment_6.ipynb#ch0000006?line=6'>7</a>\u001b[0m \u001b[39mHello AIO -2022 , we're AIVN.\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Documents/GitHub/aio-2022/assignment/solutions/quang-nguyen_vo-huynh_assignment_6.ipynb#ch0000006?line=7'>8</a>\u001b[0m \u001b[39mFollow us at: https://www.facebook.com/aivietnam.edu.vn\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Documents/GitHub/aio-2022/assignment/solutions/quang-nguyen_vo-huynh_assignment_6.ipynb#ch0000006?line=8'>9</a>\u001b[0m \u001b[39m# aivn # aio2022 \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Documents/GitHub/aio-2022/assignment/solutions/quang-nguyen_vo-huynh_assignment_6.ipynb#ch0000006?line=9'>10</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Documents/GitHub/aio-2022/assignment/solutions/quang-nguyen_vo-huynh_assignment_6.ipynb#ch0000006?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(preprocessor\u001b[39m.\u001b[39;49mpreprocess_text(text, lowercase \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m , uppercase \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m ))\n",
      "\u001b[1;32mc:\\Users\\Administrator\\Documents\\GitHub\\aio-2022\\assignment\\solutions\\quang-nguyen_vo-huynh_assignment_6.ipynb Cell 3'\u001b[0m in \u001b[0;36mTextPreprocessor.preprocess_text\u001b[1;34m(self, text, lowercase, uppercase, stemming, lemmatization, remove_url, remove_tags)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Documents/GitHub/aio-2022/assignment/solutions/quang-nguyen_vo-huynh_assignment_6.ipynb#ch0000002?line=58'>59</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess_text\u001b[39m(\u001b[39mself\u001b[39m, text, lowercase \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, uppercase \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, stemming \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, lemmatization \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, remove_url \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, remove_tags \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Documents/GitHub/aio-2022/assignment/solutions/quang-nguyen_vo-huynh_assignment_6.ipynb#ch0000002?line=59'>60</a>\u001b[0m     preprocessed_text \u001b[39m=\u001b[39m text\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Documents/GitHub/aio-2022/assignment/solutions/quang-nguyen_vo-huynh_assignment_6.ipynb#ch0000002?line=61'>62</a>\u001b[0m     \u001b[39massert\u001b[39;00m(lowercase \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m \u001b[39mand\u001b[39;00m uppercase \u001b[39m!=\u001b[39m \u001b[39mTrue\u001b[39;00m) \u001b[39mor\u001b[39;00m (lowercase \u001b[39m!=\u001b[39m \u001b[39mTrue\u001b[39;00m \u001b[39mand\u001b[39;00m uppercase \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m),\u001b[39m'\u001b[39m\u001b[39mCannot do both lowercasing and uppercasing. Please specify only one of them.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Documents/GitHub/aio-2022/assignment/solutions/quang-nguyen_vo-huynh_assignment_6.ipynb#ch0000002?line=62'>63</a>\u001b[0m     \u001b[39mif\u001b[39;00m lowercase \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Documents/GitHub/aio-2022/assignment/solutions/quang-nguyen_vo-huynh_assignment_6.ipynb#ch0000002?line=63'>64</a>\u001b[0m         preprocessed_text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlowercase(preprocessed_text)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Cannot do both lowercasing and uppercasing. Please specify only one of them."
     ]
    }
   ],
   "source": [
    "##\n",
    "# TEST:\n",
    "#\n",
    "preprocessor = TextPreprocessor()\n",
    "text = \\\n",
    "\"\"\" \n",
    "Hello AIO -2022 , we're AIVN.\n",
    "Follow us at: https://www.facebook.com/aivietnam.edu.vn\n",
    "# aivn # aio2022 \n",
    "\"\"\"\n",
    "\n",
    "print(preprocessor.preprocess_text(text, lowercase = True , uppercase = True ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Sau khi thực hiện tiền xử lý văn bản, người ta thường biểu diễn lại văn bản thành vector chứa\n",
    "các con số biểu diễn cho từng từ trong văn bản (text vectorization) nhằm giúp máy tính có thể\n",
    "thực hiện các phép tính toán cũng như sẽ giúp gia tăng hiệu suất mô hình nếu biểu diễn chúng\n",
    "đủ tốt. Kỹ thuật vector hóa văn bản sẽ bao gồm có các thành phần như sau:\n",
    "\n",
    "(a) **Tách token (tokenization)**: tách các từ (token) trong một chuỗi thành các chuỗi riêng lẻ\n",
    "dựa vào khoảng trắng và lưu chúng vào một list (vector từ).\n",
    "\n",
    "(b) **Tạo bộ từ điển (create dictionary)**: với mỗi văn bản trong bộ dữ liệu, lưu các token tách\n",
    "được từ các văn bản vào một list, nếu từ đó đã xuất hiện trong list rồi thì không cần phải\n",
    "đưa vào nữa.\n",
    "\n",
    "(c) **Mã hóa văn bản (text encoding)**: là kỹ thuật biểu diễn văn bản đầu vào thành vector\n",
    "với phần tử là các con số đại diện cho các từ bên trong văn bản.\n",
    "* Count vectorizer: giả sử với bộ từ điển gồm n từ, lúc này ta có thể tạo một vector n\n",
    "phần tử có giá trị ban đầu là 0. Sau đó, nếu một từ trong văn bản đầu vào có trong bộ\n",
    "từ điển, vị trí của từ đó trong vector n phần tử sẽ được cộng thêm 1. Như vậy, với một\n",
    "văn bản ta có thể biểu diễn được thành một vector n phần tử.\n",
    "\n",
    "Với những khái niệm về các bước vector hóa văn bản, các bạn hãy viết chương trình Python thực\n",
    "hiện khởi tạo một class ``TextVectorizer()`` với một số yêu cầu như sau:\n",
    "* Class ``TextVectorizer()`` sẽ bao gồm 5 phương thức: phương thức khởi tạo, phương thức\n",
    "``tokenize()``, phương thức ``create_dictionary()``, phương thức ``one_hot_encoding()`` và phương thức ``count_vectorizer()``. Trong class sẽ có một thuộc tính là dictionary với giá\n",
    "trị mặc định là None nếu người dùng không cung cấp bộ từ điển nào.\n",
    "* Đối với phương thức khởi tạo, cho phép người dùng truyền vào tham số dictionary (list), là\n",
    "một bộ từ điển. Nếu không chỉ định bộ từ điển, gán giá trị mặc định là None.\n",
    "* Đối với phương thức ``tokenize()``, input là một văn bản (str), output là một list các token\n",
    "(list).\n",
    "* Đối với phương thức ``create_dictionary()``, input là một list các văn bản (list các str),\n",
    "output là một từ điển (list) với các phần tử bên trong là các token đã trích được từ các văn\n",
    "bản.\n",
    "* Đối với phương thức ``count_vectorizer()``, input là một văn bản (str), output là vector\n",
    "count của văn bản đầu vào (list).\n",
    "* Đối với phương thức ``one_hot_encoding()``, input là một văn bản (str), output là vector\n",
    "one-hot của văn bản đầu vào (list các list).\n",
    "* Như vậy, Input/Output của chương trình là:\n",
    "    * Input:\n",
    "        * text cần vector hóa (bắt buộc).\n",
    "        * bộ từ điển hoặc list các văn bản dùng để tạo bộ từ điển.\n",
    "    * Output: vector one-hot của text đầu vào."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# EXERCISE 2:\n",
    "#\n",
    "class TextVectorizer():\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
